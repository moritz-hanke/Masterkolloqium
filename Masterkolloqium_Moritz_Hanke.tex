\documentclass{beamer}
\usepackage[utf8x]{inputenc}
\usepackage[ngerman]{babel}

\usepackage{apager}


\usetheme{Montpellier}  %% Themenwahl
\setbeamercolor{normal text}{fg=black,bg=white}

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\setbeamercolor{footline}{fg=black}

\AtBeginSection{\frame{\sectionpage}}
\AtBeginSubsection{\frame{\subsectionpage}}
\newtranslation[to=ngerman]{Section}{Abschnitt}
\newtranslation[to=ngerman]{Subsection}{Unterabschnitt}


\defbeamertemplate{section page}{mine}[1][]{%
  \begin{centering}
    {\usebeamerfont{section name}\usebeamercolor[fg]{section name}#1}
    \vskip1em\par
    \begin{beamercolorbox}[sep=12pt,center]{part title}
      \usebeamerfont{section title}\insertsection\par
    \end{beamercolorbox}
  \end{centering}
}

\defbeamertemplate{subsection page}{mine}[1][]{%
  \begin{centering}
    {\usebeamerfont{subsection name}\usebeamercolor[fg]{subsection name}#1}
    \vskip1em\par
    \begin{beamercolorbox}[sep=8pt,center,#1]{part title}
      \usebeamerfont{subsection title}\insertsubsection\par
    \end{beamercolorbox}
  \end{centering}
}

\subtitle{Kolloqium zur Masterarbeit}
\title[Zentralitätsmaße bei der NBPR]{Der Einfluss von Zentralitätsmaßen bei der netzwerkbasierten penalisierten Regression}
\author{von: Moritz Hanke\\
Betreuung: Prof. Dr. Iris Pigeot-Kübler \& Dr. Ronja Foraita }
\institute{Matrikelnummer: 2404575\\
Studiengang Medical Biometry/Biostatistics (M.Sc.)\\
Fachbereich 3: Mathematik, Universität Bremen\\
\includegraphics[width=0.5\textwidth,height=.19\textheight]{Logos}}
\date{\today}
 
\begin{document}
\selectlanguage{ngerman}
\maketitle
%\frame{\tableofcontents[currentsection]}
 
\section{Problemstellung und Idee}
\begin{frame} %%Eine Folie
  \frametitle{Das klassische lineare Regressionsmodell} %%Folientitel
  \begin{itemize}
  \item Responsevariable $Y$,  Kovariablen $X_1, \dots, X_p$ und Fehlerterm $\varepsilon$
  \begin{align*}\label{Eq_klassisches_modell}
  Y=\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \varepsilon
  \end{align*}
  \pause \item $x_j,\dots, x_p$ als Realisierungen der Zufallsvariable oder den deterministischen Werten für $X_1, \dots, X_p$
  \item es liegen $i=1, \dots, n$ Beobachtungen für die Responsevariable und die Kovariablen vor
  \pause \item sei $\mathbf{Y} = (Y_1,\dots,Y_n)'
  $, $
  \boldsymbol{\beta} = (\beta_0,\beta_1,\dots,\beta_p
  )'
  $, $
  \boldsymbol{\varepsilon} = (\varepsilon_1,\dots,\varepsilon_n
  )'$ un Designmatrix $\mathbf{X}$:
  \begin{align*}
  \mathbf{Y}=\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
  \end{align*}
  \item zudem sei $\mathbb{E}(\boldsymbol{\varepsilon})=\mathbf{0}$, $\mathbb{V}\text{ar}(\varepsilon_i)=\sigma^2$ und $\mathbb{C}\text{ov}(\boldsymbol{\varepsilon})=\sigma^2\mathbf{I}$
  \end{itemize}
\end{frame}

\begin{frame} %%Eine Folie
  %\frametitle{Hochdimensionale bei der klassischen linearen Regression} %%Folientitel
  \begin{itemize}
  \item zur Schätzung von $\boldsymbol{\beta}$ als \glqq Klassiker\grqq die \textit{Methode der kleinsten Quadrate} (OLS):
  \begin{align*}
  \boldsymbol{\hat{\beta}}^{OLS}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{Y}
  \end{align*}
  \pause \item der OLS-Schätzer ist unverzerrt, d.h. $\mathbb{E}(\boldsymbol{\hat{\beta}}^{OLS})=\boldsymbol{\beta}$, und hat die Varianzen $\mathbb{V}\text{ar}(\hat{\beta}_j^{OLS})=\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}_{jj}=\frac{\sigma^2}{1-R^2_j}$ mit den (multiplen) Determinationskoeffizienten $R^2_j$
  \pause \item für den mittleren quadratischen Fehler (MSE) als Genauigkeitsmaß der Schätzungen gilt deswegen:
  \begin{align*}
  \text{MSE}(\hat{\beta}_j^ {OLS})&=\mathbb{E}[(\hat{\beta}_j^ {OLS} - \beta_j)^2]\\
  &=\mathbb{V}\text{ar}(\hat{\beta}_j^ {OLS})+(Bias(\hat{\beta}_j^ {OLS}))^2=\mathbb{V}\text{ar}(\hat{\beta}_j^ {OLS})
  \end{align*}
  \end{itemize}
\end{frame}




\begin{frame} %%Eine Folie
  \frametitle{Limitationen des OLS-Schätzers} 
  ABER: 
    \begin{itemize}
    \pause \item für $n < p$ ist $\text{rang}(\mathbf{X}
    )<p+1$ und deswegen $\left(\mathbf{X}'\mathbf{X}\right)^{-1}$ nicht invertierbar
    \pause \item liegt Multikollinearität vor, d.h. $R^2_j$ ist nahe $1$, ist die Varianz der einzelnen $\hat{\beta}_j^{OLS}$ bzw. der entsprechende MSE hoch und damit der Schätzer ungenau
    \pause \item bei perfekter Multikollinearität ist $\mathbf{X}$ singulär und $\left(\mathbf{X}'\mathbf{X}\right)^{-1}$ nicht invertierbar
    \end{itemize}
    \begin{itemize}
    \pause \item diese Probleme sind bei hochdimensionalen Daten, d.h. $p \gg n$, gegeben
    \item $p \gg n$-Probleme kommen in verschiedenen Bereichen wie bei der Analyse von sozialen Netzwerken, Finanzmärkten, astronomischen Phänomenen und in der Genetik vor
    \end{itemize}
    
\end{frame}

\begin{frame}
	IDEE:
	\begin{itemize}
	\pause \item Annahme, dass $\boldsymbol{\beta}$ spärlich besetzt ist, d.h. $\beta_j=0$ für die meisten Regressionskoeffizienten gilt
	\pause \item Beispiel Genetik: Expressionslevelmessungen von mehreren tausend Genen für wenige hundert Probanden für eine Krankheit; biologisch beteiligt an der Entstehung der Krankheit sind aber nur wenige Gene
	\pause \item die Unverzerrtheit des OLS-Schätzers {\glqq}opfern{\grqq} um eindeutige und stabile $\boldsymbol{\hat{\beta}}$ zu erhalten, die wie $\boldsymbol{\beta}$ spärlich besetzt sind und möglichst geringen MSE durch Varianzverminderung aufweisen
	\end{itemize}
\end{frame}


\section{Penalisierte Regressionen}
\begin{frame} %%Eine Folie
\frametitle{Allgemeine Darstellung}
  \begin{itemize}
  \item die bzgl. $\boldsymbol{\beta}$ zu minimierende Zielfunktion $(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})$ wird um eine Nebenbedingung für die zu schätzenden Regressionskoeffizienten erweitert 
  \item Darstellung in Lagrange-Form als Summe aus Residuenquadratsumme (RSS) und Strafterm $P(\boldsymbol{\beta}, \boldsymbol{\gamma})$:
  \begin{align*}
  \text{PRSS}(\boldsymbol{\beta}, \boldsymbol{\gamma})=\text{RSS}(\boldsymbol{\beta})+P(\boldsymbol{\beta}, \boldsymbol{\gamma})
  \end{align*}
  \item im Allgemeinen wird gefordert: $\frac{1}{n}\sum_{i}^{n}y_i =0$ sowie $\frac{1}{n}\sum_{i=1}^{n}x_{ij}=0$ und $\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2=1$
  \end{itemize} 
\end{frame}


\begin{frame}
\frametitle{\glqq Klassische{\grqq} Penalisierungen ohne Vorabinformationen}
	\begin{itemize}
	\item \textit{Ridge-Regression} \cite{hoerl_ridge_1970}:
	\begin{align*}
	\boldsymbol{\hat{\beta}}^{Ridge}(\gamma)=\arg \displaystyle\min_{\boldsymbol{\beta}} \left\lbrace \text{RSS}  + \gamma \sum_{j=1}^{p}\beta_j^2 \right\rbrace \ \text{mit} \ \gamma \ge 0
	\end{align*}
	\begin{itemize}
	\item analytische Lösung: $\boldsymbol{\hat{\beta}}^{Ridge} = (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y}$
	\item es existiert eine eindeutige Lösung
	\item Stabilisierung der Schätzer\\
	ABER:\\ 
	\item alle $\hat{\beta}_j \neq 0$
	
	\end{itemize}
	\end{itemize}
\end{frame}



\begin{frame}
	\begin{itemize}
	\item \textit{Lasso-Regression} \cite{tibshirani96regression}:
	\begin{align*}
	\boldsymbol{\hat{\beta}}^{Lasso}(\gamma)=\arg \displaystyle\min_{\boldsymbol{\beta}} \left\lbrace \text{RSS}  + \gamma \sum_{j=1}^{p}|\beta_j| \right\rbrace \ \text{mit} \ \gamma \ge 0
	\end{align*}
	\begin{itemize}
	\item führt Variablenselektion durch, d.h. $\hat{\beta}_j = 0$ möglich
	\item keine analytische Lösung; Lösungsalgorithmus \textit{LARS} \cite{Efron04leastangle} effektiv\\
	ABER:\\
	\item bei hoher Multikollinearität keine eindeutige Lösung; zufällige Auswahl von hoch miteinander korrelierenden Kovariablen
	\item maximal für $n$ Kovariablen $\hat{\beta}_j \neq 0$ 
	\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
	\item \textit{Elastic-Net-Regression} \cite{zou_regularization_2005}:
	\begin{align*}
	\boldsymbol{\hat{\beta}}^{Enet}(\gamma_1, \gamma_2)=&\arg \displaystyle\min_{\beta} \left\lbrace \text{RSS}  + \gamma_1 \sum_{j=1}^{p}|\beta_j| + \gamma_2 \sum_{j=1}^{p}\beta_j^2 \right\rbrace\\ &\text{mit} \ \gamma_1,\gamma_2 \ge 0
	\end{align*}
	\end{itemize}
\end{frame}


\begin{frame}{Bibliography}
\bibliographystyle{apager}
\bibliography{Masterarbeit}\sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j)^2
\item \cite{Bonacich1987EigenvektorCent}
\end{frame}


%\begin{frame}[t,allowframebreaks] 
%	\frametitle{References}
%	\bibliography{\jobname}
%\end{frame}




\end{document}