\documentclass{beamer}
\usepackage[utf8x]{inputenc}
\usepackage[ngerman]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}


\usepackage{apager}

\usepackage{comment} 

\usetheme{Montpellier}  %% Themenwahl

\usepackage{natbib}
\bibliographystyle{apalike}
% make bibliography entries smaller
\renewcommand\bibfont{\scriptsize}
% If you have more than one page of references, you want to tell beamer
% to put the continuation section label from the second slide onwards
\setbeamertemplate{frametitle continuation}[from second]
% Now get rid of all the colours
\setbeamercolor*{bibliography entry title}{fg=black}
\setbeamercolor*{bibliography entry author}{fg=black}
\setbeamercolor*{bibliography entry location}{fg=black}
\setbeamercolor*{bibliography entry note}{fg=black}
% and kill the abominable icon
\setbeamertemplate{bibliography item}{}



\setbeamercolor{normal text}{fg=black,bg=white}

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\setbeamercolor{footline}{fg=black}

\AtBeginSection{\frame{\sectionpage}}
\AtBeginSubsection{\frame{\subsectionpage}}
\newtranslation[to=ngerman]{Section}{Abschnitt}
\newtranslation[to=ngerman]{Subsection}{Unterabschnitt}


\defbeamertemplate{section page}{mine}[1][]{%
  \begin{centering}
    {\usebeamerfont{section name}\usebeamercolor[fg]{section name}#1}
    \vskip1em\par
    \begin{beamercolorbox}[sep=12pt,center]{part title}
      \usebeamerfont{section title}\insertsection\par
    \end{beamercolorbox}
  \end{centering}
}

\defbeamertemplate{subsection page}{mine}[1][]{%
  \begin{centering}
    {\usebeamerfont{subsection name}\usebeamercolor[fg]{subsection name}#1}
    \vskip1em\par
    \begin{beamercolorbox}[sep=8pt,center,#1]{part title}
      \usebeamerfont{subsection title}\insertsubsection\par
    \end{beamercolorbox}
  \end{centering}
}

\subtitle{Kolloqium zur Masterarbeit}
\title[Zentralitätsmaße bei der NBPR]{Der Einfluss von Zentralitätsmaßen bei der netzwerkbasierten penalisierten Regression}
\author{von: Moritz Hanke}
\institute{Studiengang Medical Biometry/Biostatistics (M.Sc.)\\
Fachbereich 3: Mathematik, Universität Bremen\\
\includegraphics[width=0.5\textwidth,height=.19\textheight]{Logos}}
\date{\today}

\begin{document}
\selectlanguage{ngerman}
\maketitle
%\frame{\tableofcontents[currentsection]}

\section{Problemstellung und Idee}
\begin{frame} %%Eine Folie
  \frametitle{Das klassische lineare Regressionsmodell} %%Folientitel
  \begin{itemize}
  \item Responsevariable $Y$,  Kovariablen $X_1, \dots, X_p$ und Fehlerterm $\varepsilon \sim N(0, \sigma^2)$
  \begin{align*}\label{Eq_klassisches_modell}
  Y=\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \varepsilon
  \end{align*}
  \item Schätzung von $\boldsymbol{\beta}$ \glqq klassisch\grqq mit \textit{Methode der kleinsten Quadrate} (OLS):
  \begin{align*}
  \boldsymbol{\hat{\beta}}^{OLS}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{Y}
  \end{align*}
  \item der OLS-Schätzer ist unverzerrt, d.h. $\mathbb{E}(\boldsymbol{\hat{\beta}}^{OLS})=\boldsymbol{\beta}$, und hat die Varianzen $\mathbb{V}\text{ar}(\hat{\beta}_j^{OLS})=\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}_{jj}=\frac{\sigma^2}{1-R^2_j}$ mit den (multiplen) Determinationskoeffizienten $R^2_j$
  \item $\text{MSE}(\hat{\beta}_j^ {OLS})=\mathbb{V}\text{ar}(\hat{\beta}_j^ {OLS})+(Bias(\hat{\beta}_j^ {OLS}))^2=\mathbb{V}\text{ar}(\hat{\beta}_j^ {OLS})$
  \end{itemize}
\end{frame}





\begin{frame} %%Eine Folie
  \frametitle{Limitationen des OLS-Schätzers} 
  ABER: 
    \begin{itemize}
    \pause \item für $n < p$ ist $\text{rang}(\mathbf{X}
    )<p+1$ und deswegen $\left(\mathbf{X}'\mathbf{X}\right)^{-1}$ nicht invertierbar
    \pause \item liegt Multikollinearität vor, d.h. ist $R^2_j$ nahe $1$, ist die Varianz der einzelnen $\hat{\beta}_j^{OLS}$ bzw. der entsprechende MSE hoch und damit der Schätzer ungenau
    \pause \item bei perfekter Multikollinearität ist $\mathbf{X}$ singulär und $\left(\mathbf{X}'\mathbf{X}\right)^{-1}$ nicht invertierbar
    \end{itemize}
    \begin{itemize}
    \pause \item diese Probleme sind bei hochdimensionalen Daten, d.h. $p \gg n$, gegeben
    \item $p \gg n$-Probleme kommen in verschiedenen Bereichen wie bei der Analyse von sozialen Netzwerken, Finanzmärkten und astronomischen Phänomenen sowie in der Genetik vor
    \end{itemize}
    
\end{frame}

\begin{frame}
	IDEE:
	\begin{itemize}
	\pause \item Annahme, dass $\boldsymbol{\beta}$ spärlich besetzt ist, d.h. $\beta_j=0$ für die meisten Regressionskoeffizienten gilt
	\pause \item Beispiel Genetik: Expressionslevelmessungen von mehreren tausend Genen für wenige hundert Probanden für ein Outcome; biologisch wirklich beteiligt sind aber nur wenige Gene
	\pause \item die Unverzerrtheit des OLS-Schätzers {\glqq}opfern{\grqq} um eindeutige und stabile $\boldsymbol{\hat{\beta}}$ zu erhalten, die wie $\boldsymbol{\beta}$ spärlich besetzt sind und möglichst geringen MSE durch Varianzverminderung aufweisen
	\pause \item zusätzlich zuvor bekannte Informationen wie den Zusammenhang zwischen den Kovariablen (Netzwerk) und deren jeweilige Bedeutung bei der Schätzung berücksichtigen
	\end{itemize}
\end{frame}


\section{Penalisierte Regressionen}
\begin{frame} %%Eine Folie
\frametitle{Allgemeine Darstellung}
  \begin{itemize}
  \item die bzgl. $\boldsymbol{\beta}$ zu minimierende Zielfunktion $(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})$ wird um eine Nebenbedingung für die zu schätzenden Regressionskoeffizienten erweitert 
  \item Darstellung in Lagrange-Form als Summe aus Residuenquadratsumme (RSS) und Strafterm $P(\boldsymbol{\beta}, \boldsymbol{\gamma})$:
  \begin{align*}
  \text{PRSS}(\boldsymbol{\beta}, \boldsymbol{\gamma})=\text{RSS}(\boldsymbol{\beta})+P(\boldsymbol{\beta}, \boldsymbol{\gamma})
  \end{align*}
  \item im Allgemeinen wird gefordert: $\frac{1}{n}\sum_{i}^{n}y_i =0$ sowie $\frac{1}{n}\sum_{i=1}^{n}x_{ij}=0$ und $\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2=1$
  \end{itemize} 
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{\glqq Klassische{\grqq} Penalisierungen ohne Vorabinformationen}
	\begin{itemize}
	\item \textit{Ridge-Regression} von \cite{hoerl_ridge_1970}:
	\begin{align*}
	\boldsymbol{\hat{\beta}}^{Ridge}(\gamma)=\arg \displaystyle\min_{\boldsymbol{\beta}} \left\lbrace \text{RSS}  + \gamma \sum_{j=1}^{p}\beta_j^2 \right\rbrace \ \text{mit} \ \gamma \ge 0
	\end{align*}
	\begin{itemize}
	\item analytische Lösung: $\boldsymbol{\hat{\beta}}^{Ridge} = (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y}$
	\item es existiert eine eindeutige Lösung
	\item Stabilisierung der Schätzer\\
	ABER:\\ 
	\item alle $\hat{\beta}_j \neq 0$
	
	\end{itemize}
	\end{itemize}
\end{frame}



\begin{frame}
	\begin{itemize}
	\item \textit{Lasso-Regression} von \cite{tibshirani96regression}:
	\begin{align*}
	\boldsymbol{\hat{\beta}}^{Lasso}(\gamma)=\arg \displaystyle\min_{\boldsymbol{\beta}} \left\lbrace \text{RSS}  + \gamma \sum_{j=1}^{p}|\beta_j| \right\rbrace \ \text{mit} \ \gamma \ge 0
	\end{align*}
	\begin{itemize}
	\item führt Variablenselektion durch, d.h. $\hat{\beta}_j = 0$ möglich
	\item keine analytische Lösung; Lösungsalgorithmus \textit{LARS} \cite{Efron04leastangle} effektiv\\
	ABER:\\
	\item bei hoher Multikollinearität keine eindeutige Lösung; zufällige Auswahl von hoch miteinander korrelierenden Kovariablen
	\item maximal für $n$ Kovariablen $\hat{\beta}_j \neq 0$ 
	\end{itemize}
	\end{itemize}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{Der \glqq Goldstandard{\grqq} für $p \gg n$}
	\begin{itemize}
	\item \textit{Elastic-Net-Regression} von \cite{zou_regularization_2005}:
	\begin{align*}
	\boldsymbol{\hat{\beta}}^{Enet}(\gamma_1, \gamma_2)=&\arg \displaystyle\min_{\beta} \left\lbrace \text{RSS}  + \gamma_1 \sum_{j=1}^{p}|\beta_j| + \gamma_2 \sum_{j=1}^{p}\beta_j^2 \right\rbrace\\ &\text{mit} \ \gamma_1,\gamma_2 \ge 0
	\end{align*}
	\item Kombination der Stärken der Ridge-Regression (\cite{hoerl_ridge_1970}) und Lasso-Regression (\cite{tibshirani96regression}):
	\begin{itemize}
	\item Variablenselektion 
	\item stabile Schätzer
	\item Gruppierungseffekt
	\end{itemize}	
	\end{itemize}
\end{frame}

\begin{frame}
	Zudem: für die Differenz zweier Kovariablen
		\begin{align*}
		D_{\gamma_1,\gamma_2}(k,l)=\frac{1}{||\mathbf{Y}||_1}||\hat{\beta}_k^{Enet}(\gamma_1,\gamma_2)-\hat{\beta}_l^{Enet}(\gamma_1,\gamma_2)||_2
		\end{align*}
		gilt mit Korrelationskoeffizient $\rho$:
		\begin{align*}
		D_{\gamma_1,\gamma_2}(k,l) \leq \frac{1}{\gamma_2}\sqrt{2(1-\rho_{k,l})}
		\end{align*}
\end{frame}

\begin{frame}
	ABER:
	\begin{itemize}
	\item Enet liefert nicht das wahre Modell, wenn $\mathbf{X}$ die sogenannte Irreprestable-Condition nicht erfüllt; i.d.R. nicht überprüfbar weswegen für reale Datensätze angenommen werden muss, das sie verletzt ist (vgl. \cite{jia_2008Enet_Consistency})
	\item a priori vorliegende Informationen über Kovariablen werden nicht genutzt
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Penalisierungen auf Grundlage von Netzwerkinformationen}
	\begin{itemize}
	\item  Beispiel Genetik: Informationen über Zusammenhänge zwischen einzelnen Genen oder Genprodukten bzw. zwischen Genen und Proteinen vorhanden
	\item darstellbar mittels Graphentheorie durch Knoten und Kanten sowie Kantengewichten bzw. -richtungen
	\end{itemize}
	\textbf{Forschungshypothese in der Masterarbeit:} Aussagekräftige Knotenzentralitätsmaße für jeden Knoten verbessern die Schätzung bei einer netzwerkbasoierten penalisierten Regression weiter
\end{frame}


\begin{frame}
	\frametitle{Zentralitätsmaße {\glqq}in a nutshell{\grqq}}
	\begin{align*}
	\text{Adjazenzmatrix: } \mathbf{A}=\begin{pmatrix}
	0 & 1 & 1 & 0 & 1 \\
	1 & 0 & 0 & 1 & 0 \\
	1 & 0 & 0 & 0 & 0 \\
	0 & 1 & 0 & 0 & 1 \\
	1 & 0 & 0 & 1 & 0
	\end{pmatrix} \qquad \text{Graph }G(V,E)
	\end{align*}
	\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{./Graph_Beispiel2}
	%\caption{}
	\label{fig:graphentheorie}
	\end{figure}
\end{frame}


\begin{frame}
	\begin{itemize}
	\item sei $D(u, v)$ die Distanz, d.h. der kürzeste Weg, zwischen zwei Knoten $u$ und $v$: 
	\begin{align*}
	\text{Closeness-Zentralität: } & C(u)=\frac{1}{\sum_{v \in V \setminus u} D(u, v)}
	\end{align*}
	\item sei $\varpi(v,w|u)$ die Summe an Distanzen zwischen $v$ und $w$, auf denen $u$ liegt, und sei $\varpi(v,w)$ die gesamte Anzahl an Distanzen zwischen $v$ und $w$:
	\begin{align*}
		\text{Betweenness-Zentralität: } B(u)=\sum_{u \neq v \neq w \in V} \frac{\varpi(v,w|u)}{\varpi(v,w)}\\
	\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item sei $\mathbf{A}\mathbf{e}=\lambda_{max} \mathbf{e}$ ein Eigenwertproblem und $\lambda_{max}$ der größte Eigenwert mit zugehörigem Eigenvektor $\mathbf{e}=(e_1,\dots, e_{p})'$:
	\begin{align*}
	&\text{Eigenvektor-Zentralität: } & \lambda_{max} e_u &= \sum_{v \in V\setminus u} e_v a_{u,v} \notag \\
	&&\Leftrightarrow e_u &=\frac{1}{\lambda_{max}} \sum_{v \in V\setminus u} e_v a_{u,v}
	\end{align*}
	\item die Eigenvektor-Zentralität misst die Bedeutung eines Knotens daran, wie bedeutsam seine Nachbarn sind 
\end{itemize}
\end{frame}


\begin{frame}
\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{./hsa05212}
\caption{KEGG-Pathway für Pankreas-Karzinome}
\label{fig:hsa05212}
\end{figure}
\end{frame}


\begin{frame}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{./PancreasCancer}
\caption{Graph für Pathway von Pankreas-Karzinome}
\label{fig:pancreas}
\end{figure}
\end{frame}



\begin{frame}
	\begin{itemize}
	\item ursprüngliche netzwerkbasierte Regression von \cite{li_network-constrained_2008} mit zu minimierendem Term:
	\begin{align*}
	\text{RSS}
	+\gamma_1\sum_{j=1}^{p}|\beta_j|
	+\gamma_2 \left( \sum_{\{u,v\}} \left(\frac{\beta_u}{\sqrt{d(u)}}-\frac{\beta_v}{\sqrt{d(v)}}\right)^2 w(\{u,v\})\right)
	\end{align*}
	\item $\{u,v\}$ als ungerichtete Kante zwischen Knoten (Kovariablen) $u$ und $v$ (a priori Information; Unterschied zu Enet)
	\item $d(u)$ als Knotengrad von $u$
	\item $w(\{u,v\})$ als Kantengewichtung zwischen $u$ und $v$
	\item zweiter Strafterm bewirkt Glättung
	\item strikt konvex, deswegen Gruppierungseffekt wie bei Enet
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
	\item Weiterentwicklung (im Folgenden $\text{NBPR}$) von \cite{kim_network-based_2013} mit {\glqq}idealem{\grqq} Strafterm:
	\begin{align*}
	\gamma_1 \sum_{j=1}^{p}I(|\beta_j|\neq 0) + \gamma_2 \sum_{\{u,v\}}\left| I\left( \frac{|\beta_u|}{c_u} \neq 0 \right) - I\left( \frac{|\beta_v|}{c_v} \neq 0 \right) \right|
	\end{align*}
	\item $c_u$ bzw. $c_v$ als Knotengewichtung
	\item Indikatorfunktion problematisch, weil
	alleine der erste Starfterm schon $2^p$ mögliche Lösungen hat (nicht in Polynomialzeit lösbar)
	\item Approximation der von $I(\cdot)$ durch  $J_\tau(|z|)=\min (\frac{|z|}{\tau}, 1)$ mit Tuningparameter $\tau \rightarrow 0$
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
	\item $J_\tau(|z|)$ ist nicht konvex, Autoren verwenden deswegen einen \textit{difference convex programming-algorithm} und erhalten als Starfterm für Iterationsschritt $t$:
	\begin{align*}
	P_t(\gamma_1, \gamma_2, \tau) =& \frac{\gamma_1}{\tau} \sum_{j=1}^{p}\left(|\beta_j|\cdot I(|\hat{\beta}_j^{(t-1)}| \leq \tau)\right) \notag \\
	&+ \gamma_2 \sum_{\{u,v\}} 2\max \left[\frac{|\beta_u|}{c_u}+\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right), \right. \\
	&\phantom{{}=1111} \left.\frac{|\beta_v|}{c_v}+\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right)\right] \notag \\
	&- \frac{\gamma_2}{\tau} \sum_{\{u,v\}} \left\lbrace
	\frac{\beta_u}{c_u}\text{sign}(\hat{\beta}_u^{(t-1)}) \left[1+I\left(\frac{|\hat{\beta}_u^{(t-1)}|}{c_u}>\tau \right) \right] \right. \notag \\
	&\phantom{{}=1111} \left. + 
	\frac{\beta_v}{c_v}\text{sign}(\hat{\beta}_v^{(t-1)})\left[1+I\left(\frac{|\hat{\beta}_v^{(t-1)}|}{c_v}>\tau \right) \right]
	\right\rbrace
	\end{align*}
	\end{itemize}
\end{frame}


\section{Simualtionsstudie}
\begin{frame}
	\frametitle{Simulationsdesign}
	\begin{itemize}
	\item $100$ Datensätze
	\item Simulations realistischer Graphen auf Grundlage von KEGG-Datenbank:
	\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{./Beispielgraph}
	%\caption{}
	\label{fig:beispiel}
	\end{figure}
	\item $p=1000$, $n=300$ 
	\begin{itemize}
	\item $n_{tune}=50$ zur Suche der optimalen Tuningparameter
	\item $n_{train}=50$ für Schätzung von $\boldsymbol{\beta}$
	\item $n_{test}=200$ für Beurteilung der Güte
	\end{itemize}
	\item $\beta_1,\dots,\beta_{20}=2$, $\beta_{21},\dots,\beta_{50}=1$, restlichen $\beta_j=0$
	\item $\varepsilon_i \sim N(0,8)$  
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Schätzung der Regressionskoeffizienten}
	\begin{itemize}
	\item für Enet:
	\begin{itemize}
	\item alternative Darstellung Enet:
	\begin{align*}
	\text{RSS}+\gamma \alpha \sum_{j=1}^{p}|\beta_j| + \gamma (1-\alpha) \sum_{j=1}^{p}\beta_j^2
	\end{align*}
	\item Parameter $\alpha$ aus Sequenz $\alpha={0.05, 0.1, \dots ,0.95}$; $\gamma$ aus Sequenz, die automatisch von \texttt{R}-Funktion \texttt{cv.glmnet} gewählt wird 
	\item optimale Parameterkombination mittels zehnfacher Kreuzvalidierung gewählt
	\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
	\item für $\text{NBPR}$:
	\begin{itemize}
	\item Knotengewichte mit unterschiedlichen Zentralitätsmaßen:
	\begin{itemize}
	\item Rangplätze ermittelt aus Betweenness-, Closeness- und Eigenvektor-Zentralität sowie Knotengrad ($\text{NBPR}_{B,R}$, $\text{NBPR}_{C,R}$, $\text{NBPR}_{E,R}$ und $\text{NBPR}_{d,R}$)
	\item Wurzel aus Knotengrad ($\text{NBPR}_{\sqrt{d}}$) und Closeneness-Zentralität ($\text{NBPR}_{C}$)
	\item keine Gewichtung ($\text{NBPR}_{1}$)
	\end{itemize}
	\item Enet als Startschätzer für $t=1$
	\item Wahl der Tuningparameter über kleinsten Vorhersagefehler für Kombinationen aus $4\cdot 4 \cdot 5 =80$ von $\gamma_1$, $\gamma_2$ und $\tau$:
	\begin{align*}
	&\gamma_1/\tau \in \left[ \max(|\boldsymbol{\hat{\beta}}^{Enet}|), \max(|\boldsymbol{\hat{\beta}}^{Enet}|) \cdot \frac{p}{4} \right] \\
	&\gamma_2/\tau \in \left[ N_E, \max(|\boldsymbol{\hat{\beta}}^{Enet}|) \cdot N_E \right]\\
	&\tau \in \left[0.000001, \frac{N_E}{2}\right]
	\end{align*}
	\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Ergebnisse}
	\begin{itemize}
	\item numerische Stabilität:
	\begin{itemize}
	\item Enet liefert bei allen $100$ Simulationsdurchläufen stabile Schätzer
	\item Anteil an Simulationsdurchläufen mit stabilen Schätzern für NBPR zwischen \colorbox{red}{$74\%$} ($\text{NBPR}_{C,R}$) und \colorbox{red}{$38\%$} ($\text{NBPR}_{1}$)
	\item numerisch instabile Schätzer liegen in Intervallen $[-2000000,-1000]$ bzw. $[1000,8000000]$
	\end{itemize}
	\item durchschnittliche richtig-positiv-Rate:
	\begin{itemize}
	\item Enet niedrigste Rate (ca. $43\%$)
	\item NBPR-Versionen zwischen $44\%$ ($\text{NBPR}_{C}$) und $49\%$ ($\text{NBPR}_{\sqrt{d}}$)
	\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
	\item durchschnittliche falsch-positiv-Rate:
	\begin{itemize}
	\item Enet mit zweitkleinstem Wert ($5.2\%$)
	\item NBPR-Versionen zwischen $4.8\%$ ($\text{NBPR}_{C}$) und $9.4\%$ ($\text{NBPR}_{\sqrt{d}}$)
	\item im \textbf{Durchschnitt über alle Simulationen} geht eine Erhöhung der RP-Rate mit einer Erhöhung der FP-Rate einher; \textbf{ABER:} werden die Simulationen individuell betrachtet, gibt es keinen eindeutigen Richtungszusammenhang  
	\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
	\item durchschnittliche Modellfehler $(\boldsymbol{\hat{\beta}}- \boldsymbol{\beta})'\mathbb{C}\text{ov}(\mathbf{X})(\boldsymbol{\hat{\beta}}- \boldsymbol{\beta})$:
	\begin{itemize}
	\item Enet: $155.56$
	\item NBPR-Versionen: zwischen $182.71$ ($\text{NBPR}_{E,R}$) und $236.30$ ($\text{NBPR}_{\sqrt{d}}$)
	\item NBPR-Versionen haben teilweise hohe Standardabweichungen durch potentielle Ausreißer
	\end{itemize}
	\item durchschnittliche Vorhersagefehler $\frac{1}{n}(\mathbf{X}\boldsymbol{\hat{\beta}}- \mathbf{Y})'(\mathbf{X}\boldsymbol{\hat{\beta}}- \mathbf{Y})$:
	\begin{itemize}
	\item Enet: $238.2$
	\item NBPR-Versionen deutlich höher: $886.66$ ($\text{NBPR}_{B,R}$) bis $1044.30$ ($\text{NBPR}_{\sqrt{d}}$)
	\item einzelne Vorhersagefehler von NBPR streuen breiter als bei der Enet-Regression 
	\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
	\item durchschnittliche Genauigkeit der Schätzungen für $\boldsymbol{\beta} = 2$, $\boldsymbol{\beta} = 1$ bzw $\boldsymbol{\beta} = 0$
	\begin{itemize}
	\item alle Methoden unterschätzen $\boldsymbol{\beta}=2$, wobei Enet mit $1.740$ den schlechtesten und $\text{NBPR}_1$ mit $1.919$ den besten Wert liefert
	\item für $\boldsymbol{\beta} = 1$ trifft Enet im Durchschnitt fast den wahren Wert ($1.010$); die NBPR-Versionen liegen zwischen $1.239$ ($\text{NBPR}_{B,R}$) und $1.419$ ($\text{NBPR}_1$)  
	\item für $\boldsymbol{\beta}\neq 0$ liefert Enet in Fällen wo $\hat{\beta}_j \neq 0$ gesetzt wird mit $\approx0.5$ die besten Ergebnisse, alle NBPR-Versionen liegen bei $\approx 1$
	\end{itemize}
	\end{itemize}
\end{frame}


\section{Fazit und Ausblick}
\begin{frame}

\end{frame}


\begin{frame}
Vielen Dank für die Aufmerksamkeit
\end{frame}


\section{Zusätzliches Material}
\begin{frame}
	\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{./BR_allBETA}
	\caption{Kovariablenauswahl Enet vs. $\text{NBPR}_{B,R}$ für $\boldsymbol{\beta}$}
	\label{fig:a}
	\end{figure}
\end{frame}
\begin{frame}
	\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{./BR_BETA_not0}
	\caption{Kovariablenauswahl Enet vs. $\text{NBPR}_{B,R}$ für $\boldsymbol{\beta} \neq 0$}
	\label{fig:b}
	\end{figure}
\end{frame}
\begin{frame}
	\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{./BR_BETA_0}
	\caption{Kovariablenauswahl Enet vs. $\text{NBPR}_{B,R}$ für $\boldsymbol{\beta} = 0$}
	\label{fig:c}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Verwendete Software (jeweils aktuelle Version zum Stand 17.12.2014)}
	\begin{itemize}
	\item \texttt{R}-Pakete: \texttt{igraph, corpcor, qpgraph, mvtnorm, CVXfromR, glmnet, parallel, KEGGgraph, NCBI2R}
	\item \texttt{Matlab}-Toolboxen: \texttt{cvx}
	\end{itemize}
\end{frame}


%\begin{frame}{Bibliography}
%\bibliographystyle{apager}
%\bibliography{Masterarbeit}
%\end{frame}


%\begin{frame}[t,allowframebreaks]
%\frametitle{References}
%\bibliography{\jobname}
%\end{frame}




\end{document}