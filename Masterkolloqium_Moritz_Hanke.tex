\documentclass{beamer}
\usepackage[utf8x]{inputenc}
\usepackage[ngerman]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}


\usepackage{apager}

\usepackage{comment} 

\usetheme{Montpellier}  %% Themenwahl

\usepackage{natbib}
\bibliographystyle{apalike}
% make bibliography entries smaller
\renewcommand\bibfont{\scriptsize}
% If you have more than one page of references, you want to tell beamer
% to put the continuation section label from the second slide onwards
\setbeamertemplate{frametitle continuation}[from second]
% Now get rid of all the colours
\setbeamercolor*{bibliography entry title}{fg=black}
\setbeamercolor*{bibliography entry author}{fg=black}
\setbeamercolor*{bibliography entry location}{fg=black}
\setbeamercolor*{bibliography entry note}{fg=black}
% and kill the abominable icon
\setbeamertemplate{bibliography item}{}



\setbeamercolor{normal text}{fg=black,bg=white}

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\setbeamercolor{footline}{fg=black}

\AtBeginSection{\frame{\sectionpage}}
\AtBeginSubsection{\frame{\subsectionpage}}
\newtranslation[to=ngerman]{Section}{Abschnitt}
\newtranslation[to=ngerman]{Subsection}{Unterabschnitt}


\defbeamertemplate{section page}{mine}[1][]{%
  \begin{centering}
    {\usebeamerfont{section name}\usebeamercolor[fg]{section name}#1}
    \vskip1em\par
    \begin{beamercolorbox}[sep=12pt,center]{part title}
      \usebeamerfont{section title}\insertsection\par
    \end{beamercolorbox}
  \end{centering}
}

\defbeamertemplate{subsection page}{mine}[1][]{%
  \begin{centering}
    {\usebeamerfont{subsection name}\usebeamercolor[fg]{subsection name}#1}
    \vskip1em\par
    \begin{beamercolorbox}[sep=8pt,center,#1]{part title}
      \usebeamerfont{subsection title}\insertsubsection\par
    \end{beamercolorbox}
  \end{centering}
}

\subtitle{Kolloqium zur Masterarbeit}
\title[Zentralitätsmaße bei der NBPR]{Der Einfluss von Zentralitätsmaßen bei der netzwerkbasierten penalisierten Regression}
\author{von: Moritz Hanke\\
Betreuung: Prof. Dr. Iris Pigeot-Kübler \& Dr. Ronja Foraita }
\institute{Matrikelnummer: 2404575\\
Studiengang Medical Biometry/Biostatistics (M.Sc.)\\
Fachbereich 3: Mathematik, Universität Bremen\\
\includegraphics[width=0.5\textwidth,height=.19\textheight]{Logos}}
\date{\today}

\begin{document}
\selectlanguage{ngerman}
\maketitle
%\frame{\tableofcontents[currentsection]}

\section{Problemstellung und Idee}
\begin{frame} %%Eine Folie
  \frametitle{Das klassische lineare Regressionsmodell} %%Folientitel
  \begin{itemize}
  \item Responsevariable $Y$,  Kovariablen $X_1, \dots, X_p$ und Fehlerterm $\varepsilon$
  \begin{align*}\label{Eq_klassisches_modell}
  Y=\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \varepsilon
  \end{align*}
  \pause \item $x_j,\dots, x_p$ als Realisierungen der Zufallsvariable oder den deterministischen Werten für $X_1, \dots, X_p$
  \item es liegen $i=1, \dots, n$ Beobachtungen für die Responsevariable und die Kovariablen vor
  \pause \item sei $\mathbf{Y} = (Y_1,\dots,Y_n)'
  $, $
  \boldsymbol{\beta} = (\beta_0,\beta_1,\dots,\beta_p
  )'
  $, $
  \boldsymbol{\varepsilon} = (\varepsilon_1,\dots,\varepsilon_n
  )'$ und Designmatrix $\mathbf{X}$:
  \begin{align*}
  \mathbf{Y}=\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
  \end{align*}
  \item zudem sei $\mathbb{E}(\boldsymbol{\varepsilon})=\mathbf{0}$, $\mathbb{V}\text{ar}(\varepsilon_i)=\sigma^2$ und $\mathbb{C}\text{ov}(\boldsymbol{\varepsilon})=\sigma^2\mathbf{I}$
  \end{itemize}
\end{frame}

\begin{frame} %%Eine Folie
  %\frametitle{Hochdimensionale bei der klassischen linearen Regression} %%Folientitel
  \begin{itemize}
  \item zur Schätzung von $\boldsymbol{\beta}$ als \glqq Klassiker\grqq die \textit{Methode der kleinsten Quadrate} (OLS):
  \begin{align*}
  \boldsymbol{\hat{\beta}}^{OLS}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{Y}
  \end{align*}
  \pause \item der OLS-Schätzer ist unverzerrt, d.h. $\mathbb{E}(\boldsymbol{\hat{\beta}}^{OLS})=\boldsymbol{\beta}$, und hat die Varianzen $\mathbb{V}\text{ar}(\hat{\beta}_j^{OLS})=\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}_{jj}=\frac{\sigma^2}{1-R^2_j}$ mit den (multiplen) Determinationskoeffizienten $R^2_j$
  \pause \item für den mittleren quadratischen Fehler (MSE) als Genauigkeitsmaß der Schätzungen gilt deswegen:
  \begin{align*}
  \text{MSE}(\hat{\beta}_j^ {OLS})&=\mathbb{E}[(\hat{\beta}_j^ {OLS} - \beta_j)^2]\\
  &=\mathbb{V}\text{ar}(\hat{\beta}_j^ {OLS})+(Bias(\hat{\beta}_j^ {OLS}))^2=\mathbb{V}\text{ar}(\hat{\beta}_j^ {OLS})
  \end{align*}
  \end{itemize}
\end{frame}




\begin{frame} %%Eine Folie
  \frametitle{Limitationen des OLS-Schätzers} 
  ABER: 
    \begin{itemize}
    \pause \item für $n < p$ ist $\text{rang}(\mathbf{X}
    )<p+1$ und deswegen $\left(\mathbf{X}'\mathbf{X}\right)^{-1}$ nicht invertierbar
    \pause \item liegt Multikollinearität vor, d.h. $R^2_j$ ist nahe $1$, ist die Varianz der einzelnen $\hat{\beta}_j^{OLS}$ bzw. der entsprechende MSE hoch und damit der Schätzer ungenau
    \pause \item bei perfekter Multikollinearität ist $\mathbf{X}$ singulär und $\left(\mathbf{X}'\mathbf{X}\right)^{-1}$ nicht invertierbar
    \end{itemize}
    \begin{itemize}
    \pause \item diese Probleme sind bei hochdimensionalen Daten, d.h. $p \gg n$, gegeben
    \item $p \gg n$-Probleme kommen in verschiedenen Bereichen wie bei der Analyse von sozialen Netzwerken, Finanzmärkten, astronomischen Phänomenen und in der Genetik vor
    \end{itemize}
    
\end{frame}

\begin{frame}
	IDEE:
	\begin{itemize}
	\pause \item Annahme, dass $\boldsymbol{\beta}$ spärlich besetzt ist, d.h. $\beta_j=0$ für die meisten Regressionskoeffizienten gilt
	\pause \item Beispiel Genetik: Expressionslevelmessungen von mehreren tausend Genen für wenige hundert Probanden für eine Krankheit; biologisch beteiligt an der Entstehung der Krankheit sind aber nur wenige Gene
	\pause \item die Unverzerrtheit des OLS-Schätzers {\glqq}opfern{\grqq} um eindeutige und stabile $\boldsymbol{\hat{\beta}}$ zu erhalten, die wie $\boldsymbol{\beta}$ spärlich besetzt sind und möglichst geringen MSE durch Varianzverminderung aufweisen
	\pause \item zusätzlich zuvor bekannte Informationen wie Zusammenhang zwischen den Kovariablen (Netzwerk) bei der Schätzung berücksichtigen
	\end{itemize}
\end{frame}


\section{Penalisierte Regressionen}
\begin{frame} %%Eine Folie
\frametitle{Allgemeine Darstellung}
  \begin{itemize}
  \item die bzgl. $\boldsymbol{\beta}$ zu minimierende Zielfunktion $(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})$ wird um eine Nebenbedingung für die zu schätzenden Regressionskoeffizienten erweitert 
  \item Darstellung in Lagrange-Form als Summe aus Residuenquadratsumme (RSS) und Strafterm $P(\boldsymbol{\beta}, \boldsymbol{\gamma})$:
  \begin{align*}
  \text{PRSS}(\boldsymbol{\beta}, \boldsymbol{\gamma})=\text{RSS}(\boldsymbol{\beta})+P(\boldsymbol{\beta}, \boldsymbol{\gamma})
  \end{align*}
  \item im Allgemeinen wird gefordert: $\frac{1}{n}\sum_{i}^{n}y_i =0$ sowie $\frac{1}{n}\sum_{i=1}^{n}x_{ij}=0$ und $\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2=1$
  \end{itemize} 
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{\glqq Klassische{\grqq} Penalisierungen ohne Vorabinformationen}
	\begin{itemize}
	\item \textit{Ridge-Regression} von \cite{hoerl_ridge_1970}:
	\begin{align*}
	\boldsymbol{\hat{\beta}}^{Ridge}(\gamma)=\arg \displaystyle\min_{\boldsymbol{\beta}} \left\lbrace \text{RSS}  + \gamma \sum_{j=1}^{p}\beta_j^2 \right\rbrace \ \text{mit} \ \gamma \ge 0
	\end{align*}
	\begin{itemize}
	\item analytische Lösung: $\boldsymbol{\hat{\beta}}^{Ridge} = (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y}$
	\item es existiert eine eindeutige Lösung
	\item Stabilisierung der Schätzer\\
	ABER:\\ 
	\item alle $\hat{\beta}_j \neq 0$
	
	\end{itemize}
	\end{itemize}
\end{frame}



\begin{frame}
	\begin{itemize}
	\item \textit{Lasso-Regression} von \cite{tibshirani96regression}:
	\begin{align*}
	\boldsymbol{\hat{\beta}}^{Lasso}(\gamma)=\arg \displaystyle\min_{\boldsymbol{\beta}} \left\lbrace \text{RSS}  + \gamma \sum_{j=1}^{p}|\beta_j| \right\rbrace \ \text{mit} \ \gamma \ge 0
	\end{align*}
	\begin{itemize}
	\item führt Variablenselektion durch, d.h. $\hat{\beta}_j = 0$ möglich
	\item keine analytische Lösung; Lösungsalgorithmus \textit{LARS} \cite{Efron04leastangle} effektiv\\
	ABER:\\
	\item bei hoher Multikollinearität keine eindeutige Lösung; zufällige Auswahl von hoch miteinander korrelierenden Kovariablen
	\item maximal für $n$ Kovariablen $\hat{\beta}_j \neq 0$ 
	\end{itemize}
	\end{itemize}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{Der \glqq Goldstandard{\grqq} für $p \gg n$}
	\begin{itemize}
	\item \textit{Elastic-Net-Regression} von \cite{zou_regularization_2005}:
	\begin{align*}
	\boldsymbol{\hat{\beta}}^{Enet}(\gamma_1, \gamma_2)=&\arg \displaystyle\min_{\beta} \left\lbrace \text{RSS}  + \gamma_1 \sum_{j=1}^{p}|\beta_j| + \gamma_2 \sum_{j=1}^{p}\beta_j^2 \right\rbrace\\ &\text{mit} \ \gamma_1,\gamma_2 \ge 0
	\end{align*}
	\item Kombination der Stärken der Ridge-Regression (\cite{hoerl_ridge_1970}) und Lasso-Regression (\cite{tibshirani96regression}):
	\begin{itemize}
	\item Variablenselektion 
	\item stabile Schätzer
	\item Gruppierungseffekt
	\end{itemize}	
	\end{itemize}
\end{frame}

\begin{frame}
	Zudem: für die Differenz zweier Kovariablen
		\begin{align*}
		D_{\gamma_1,\gamma_2}(k,l)=\frac{1}{||\mathbf{Y}||_1}||\hat{\beta}_k^{Enet}(\gamma_1,\gamma_2)-\hat{\beta}_l^{Enet}(\gamma_1,\gamma_2)||_2
		\end{align*}
		gilt mit Korrelationskoeffizient $\rho$:
		\begin{align*}
		D_{\gamma_1,\gamma_2}(k,l) \leq \frac{1}{\gamma_2}\sqrt{2(1-\rho_{k,l})}
		\end{align*}
\end{frame}

\begin{frame}
	ABER:
	\begin{itemize}
	\item Enet liefert nicht das wahre Modell, wenn $\mathbf{X}$ die sogenannte Irreprestable-Condition nicht erfüllt, was sich i.d.R.nicht überprüfen lässt und deswegen für die reale Datensätze angenommen werden muss (vgl. \cite{jia_2008Enet_Consistency})
	\item a priori vorliegende Informationen über Kovariablen werden nicht genutzt
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Penalisierungen auf Grundlage von Netzwerkinformationen}
	\begin{itemize}
	\item  Beispiel Genetik: Informationen über Zusammenhänge zwischen einzelnen Genen oder Genprodukten bzw. zwischen Genen und Proteinen vorhanden
	\item darstellbar mittels Graphentheorie durch Knoten und Kanten sowie Kantengewichten bzw. -richtungen
	\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{../Text/hsa05212}
%\caption{}
\label{fig:hsa05212}
\end{figure}
\end{frame}


\begin{frame}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{./PancreasCancer}
%\caption{}
\label{fig:pancreas}
\end{figure}
\end{frame}

\begin{frame}
	\begin{itemize}
	\item ursprüngliche graphenbasierte Regression von \cite{li_network-constrained_2008} mit zu minimierendem Term:
	\begin{align*}
	\text{RSS}
	+\gamma_1||\boldsymbol{\beta}||_1
	+\gamma_2 \left( \sum_{\{u,v\}} \left(\frac{\beta_u}{\sqrt{d(u)}}-\frac{\beta_v}{\sqrt{d(v)}}\right)^2 w(\{u,v\})\right)
	\end{align*}
	\item $\{u,v\}$ als ungerichtete Kante zwischen Knoten (Kovariable) $u$ und $v$ (a priori Information; Unterschied zu Enet)
	\item $d(u)$ als Knotengrad von $u$
	\item $w(\{u,v\})$ als Kantengewichtung zwischen $u$ und $v$
	\item zweiter Strafterm bewirkt Glättung
	\item strikt konvex, deswegen Gruppierungseffekt wie bei Enet
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
	\item Weiterentwicklung (i.F. $\text{NBPR}$) von \cite{kim_network-based_2013} mit {\glqq}idealem{\grqq} Strafterm:
	\begin{align*}
	\gamma_1 \sum_{j=1}^{p}I(|\beta_j|\neq 0) + \gamma_2 \sum_{\{u,v\}}\left| I\left( \frac{|\beta_u|}{c_u} \neq 0 \right) - I\left( \frac{|\beta_v|}{c_v} \neq 0 \right) \right|
	\end{align*}
	\item $c_u$ bzw. $c_v$ als Knotengewichtung
	\item Indikatorfunktion problematisch, weil
	alleine der erste Starfterm schon $2^p$ mögliche Lösungen hat (nicht in Polynomialzeit lösbar)
	\item Approximation der von $I(\cdot)$ durch  $J_\tau(|z|)=\min (\frac{|z|}{\tau}, 1)$ mit Tuningparameter $\tau \rightarrow 0$
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
	\item $J_\tau(|z|)$ ist nicht konvex, Autoren verwenden deswegen einen \textit{difference convex programming-algorithm} und erhalten als Starfterm für Iterationsschritt $t$:
	\begin{align*}
	P_t(\gamma_1, \gamma_2, \tau) =& \frac{\gamma_1}{\tau} \sum_{j=1}^{p}\left(|\beta_j|\cdot I(|\hat{\beta}_j^{(t-1)}| \leq \tau)\right) \notag \\
	&+ \gamma_2 \sum_{\{u,v\}} 2\max \left[\frac{|\beta_u|}{c_u}+\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right), \right. \\
	&\phantom{{}=1111} \left.\frac{|\beta_v|}{c_v}+\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right)\right] \notag \\
	&- \frac{\gamma_2}{\tau} \sum_{\{u,v\}} \left\lbrace
	\frac{\beta_u}{c_u}\text{sign}(\hat{\beta}_u^{(t-1)}) \left[1+I\left(\frac{|\hat{\beta}_u^{(t-1)}|}{c_u}>\tau \right) \right] \right. \notag \\
	&\phantom{{}=1111} \left. + 
	\frac{\beta_v}{c_v}\text{sign}(\hat{\beta}_v^{(t-1)})\left[1+I\left(\frac{|\hat{\beta}_v^{(t-1)}|}{c_v}>\tau \right) \right]
	\right\rbrace
	\end{align*}
	\end{itemize}
\end{frame}


\section{Simualtionsstudie}
\begin{frame}
	\frametitle{Simulationsdesign}
	\begin{itemize}
	\item auf Grundlage der KEGG-Datenbank mittels \texttt{R} Simulation realistischerer Graphen als bei \cite{kim_network-based_2013}:
	\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{./Beispielgraph}
	%\caption{}
	\label{fig:beispiel}
	\end{figure}
	\item $100$ Datensätze
	\item $p=1000$
	\item $n=300$ 
	\begin{itemize}
	\item $50$ zur Suche der optimalen Tuningparameter durch Kreuzvalidierung (Kombination aus \texttt{R} und \texttt{Matlab})
	\item $50$ für Schätzung von $\boldsymbol{\beta}$ (Kombination aus \texttt{R} und \texttt{Matlab})
	\item $200$ für Beurteilung der Güte (\texttt{R})
	\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Schätzung der Regressionskoeffizienten}
	\begin{itemize}
	\item für Enet:
	\begin{itemize}
	\item alternative Darstellung Enet:
	\begin{align*}
	\text{RSS}+\gamma \alpha \sum_{j=1}^{p}|\beta_j| + \gamma (1-\alpha) \sum_{j=1}^{p}\beta_j^2
	\end{align*}
	\item Parameter $\alpha$ aus Sequenz $\alpha={0.05, 0.1, \dots ,0.95}$; $\gamma$ aus Sequenz, die automatisch von \texttt{R}-Funktion \texttt{cv.glmnet} gewählt wird 
	\item optimale Parameterkombinationmittels zehnfacher Kreuzvalidierung gewählt
	\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
	\item für $\text{NBPR}$:
	\begin{itemize}
	\item Knotengewichte mit unterschiedlichen Zentralitätsmaßen:
	\begin{itemize}
	\item Rangplätze ermittelt aus Betweenness-, Closeness- und Eigenvektor-Zentralität sowie Knotengrad ($\text{NBPR}_{B,R}$, $\text{NBPR}_{C,R}$, $\text{NBPR}_{E,R}$ und $\text{NBPR}_{d,R}$)
	\item Wurzel aus Knotengrad ($\text{NBPR}_{\sqrt{d}}$) und Closeneness-Zentralität ($\text{NBPR}_{C}$)
	\item keine Gewichtung ($\text{NBPR}_{1}$)
	\end{itemize}
	\item Enet als Startschätzer für $t=1$
	\item Wahl der Tuningparameter über kleinsten Vorhersagefehler für Kombination aus $4\cdot 4 \cdot 5 =80$ von $\gamma_1$, $\gamma_2$ und $\tau$:
	\begin{align*}
	&\gamma_1/\tau \in \left[ \max(|\boldsymbol{\hat{\beta}}^{Enet}|), \max(|\boldsymbol{\hat{\beta}}^{Enet}|) \cdot \frac{p}{4} \right] \\
	&\gamma_2/\tau \in \left[ N_E, \max(|\boldsymbol{\hat{\beta}}^{Enet}|) \cdot N_E \right]\\
	&\tau \in \left[0.000001, \frac{N_E}{2}\right]
	\end{align*}
	\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Ergebnisse}
	\begin{itemize}
	\item numerische Stabilität
	\end{itemize}
\end{frame}



\begin{frame}
	\frametitle{Verwendete Software}
	\begin{itemize}
	\item \texttt{R}-Pakete: \texttt{igraph, corpcor, qpgraph, mvtnorm, CVXfromR, glmnet, parallel, KEGGgraph, NCBI2R}
	\item \texttt{Matlab}-Toolboxen: \mathtt{cvx}
	\end{itemize}
\end{frame}


%\begin{frame}{Bibliography}
%\bibliographystyle{apager}
%\bibliography{Masterarbeit}
%\end{frame}


%\begin{frame}[t,allowframebreaks]
%\frametitle{References}
%\bibliography{\jobname}
%\end{frame}




\end{document}